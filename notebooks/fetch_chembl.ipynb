{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57dfdf-87e5-4dd7-9fcf-7799e601beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from datetime import datetime\n",
    "\n",
    "def estimate_chembl_assays():\n",
    "    \"\"\"\n",
    "    Estimate ChEMBL assay counts using different methods\n",
    "    Since .count() doesn't exist, we'll use sampling and binary search\n",
    "    \"\"\"\n",
    "    print(\"=== ChEMBL Assay Size Estimator ===\")\n",
    "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    assay = new_client.assay\n",
    "    \n",
    "    # Method 1: Binary search to find approximate size\n",
    "    print(\"Method 1: Binary Search Estimation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    target_filter = {\n",
    "        'assay_type': 'A',\n",
    "        'assay_organism__iexact': 'Homo sapiens'\n",
    "    }\n",
    "    \n",
    "    estimated_size = binary_search_size(assay, target_filter)\n",
    "    print(f\"Estimated Human Type A assays: ~{estimated_size:,} records\")\n",
    "    print()\n",
    "    \n",
    "    # Method 2: Sample at different offsets\n",
    "    print(\"Method 2: Offset Sampling\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sample_sizes = sample_at_offsets(assay, target_filter)\n",
    "    print()\n",
    "    \n",
    "    # Method 3: Progressive sampling\n",
    "    print(\"Method 3: Progressive Sampling\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    progressive_estimate = progressive_sampling(assay, target_filter)\n",
    "    print()\n",
    "    \n",
    "    # Summary and recommendations\n",
    "    print(\"=== SUMMARY ===\")\n",
    "    estimates = [estimated_size, progressive_estimate]\n",
    "    estimates = [e for e in estimates if e is not None]\n",
    "    \n",
    "    if estimates:\n",
    "        min_est = min(estimates)\n",
    "        max_est = max(estimates)\n",
    "        avg_est = sum(estimates) / len(estimates)\n",
    "        \n",
    "        print(f\"Size estimates range: {min_est:,} - {max_est:,} records\")\n",
    "        print(f\"Average estimate: {avg_est:,.0f} records\")\n",
    "        \n",
    "        # File size and time estimates\n",
    "        estimate_download_specs(avg_est)\n",
    "    else:\n",
    "        print(\"Could not get reliable estimates\")\n",
    "    \n",
    "    return estimates\n",
    "\n",
    "def binary_search_size(assay, filter_params):\n",
    "    \"\"\"\n",
    "    Use binary search to find approximate dataset size\n",
    "    \"\"\"\n",
    "    print(\"Using binary search to estimate total size...\")\n",
    "    \n",
    "    # Start with a range\n",
    "    low = 0\n",
    "    high = 1000000  # Start with 1M as upper bound\n",
    "    \n",
    "    # First, find an upper bound\n",
    "    print(\"Finding upper bound...\")\n",
    "    while True:\n",
    "        try:\n",
    "            result = assay.filter(**filter_params).only(['assay_chembl_id'])[high-1:high]\n",
    "            result_list = list(result)\n",
    "            \n",
    "            if len(result_list) == 0:\n",
    "                print(f\"  Upper bound found: {high:,}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  {high:,} records exist, trying higher...\")\n",
    "                high *= 2\n",
    "                \n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error at {high:,}: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Now binary search for exact size\n",
    "    print(\"Binary searching for exact size...\")\n",
    "    last_valid = 0\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        \n",
    "        try:\n",
    "            result = assay.filter(**filter_params).only(['assay_chembl_id'])[mid-1:mid]\n",
    "            result_list = list(result)\n",
    "            \n",
    "            if len(result_list) > 0:\n",
    "                last_valid = mid\n",
    "                low = mid + 1\n",
    "                print(f\"  Record exists at {mid:,}, searching higher...\")\n",
    "            else:\n",
    "                high = mid - 1\n",
    "                print(f\"  No record at {mid:,}, searching lower...\")\n",
    "                \n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error at {mid:,}: {e}\")\n",
    "            high = mid - 1\n",
    "    \n",
    "    return last_valid\n",
    "\n",
    "def sample_at_offsets(assay, filter_params):\n",
    "    \"\"\"\n",
    "    Sample at different offsets to estimate size\n",
    "    \"\"\"\n",
    "    print(\"Sampling at different offsets...\")\n",
    "    \n",
    "    offsets_to_test = [0, 10000, 50000, 100000, 500000]\n",
    "    sample_size = 100\n",
    "    \n",
    "    valid_offsets = []\n",
    "    \n",
    "    for offset in offsets_to_test:\n",
    "        try:\n",
    "            print(f\"  Testing offset {offset:,}...\")\n",
    "            \n",
    "            result = assay.filter(**filter_params).only(['assay_chembl_id'])[offset:offset + sample_size]\n",
    "            result_list = list(result)\n",
    "            \n",
    "            if len(result_list) > 0:\n",
    "                valid_offsets.append(offset)\n",
    "                print(f\"    ✓ Got {len(result_list)} records\")\n",
    "            else:\n",
    "                print(f\"    ❌ No records found\")\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    if valid_offsets:\n",
    "        # Estimate based on highest valid offset\n",
    "        max_valid = max(valid_offsets)\n",
    "        # Add some buffer since we don't know exactly where it ends\n",
    "        estimated = max_valid + 50000\n",
    "        print(f\"  Estimate based on sampling: >{max_valid:,}, probably ~{estimated:,}\")\n",
    "        return estimated\n",
    "    \n",
    "    return None\n",
    "\n",
    "def progressive_sampling(assay, filter_params):\n",
    "    \"\"\"\n",
    "    Keep sampling until we hit the end\n",
    "    \"\"\"\n",
    "    print(\"Progressive sampling to find end...\")\n",
    "    \n",
    "    batch_size = 1000\n",
    "    offset = 0\n",
    "    last_successful_offset = 0\n",
    "    consecutive_empty = 0\n",
    "    \n",
    "    while consecutive_empty < 3:  # Stop after 3 consecutive empty batches\n",
    "        try:\n",
    "            print(f\"  Sampling at offset {offset:,}...\")\n",
    "            \n",
    "            result = assay.filter(**filter_params).only(['assay_chembl_id'])[offset:offset + batch_size]\n",
    "            result_list = list(result)\n",
    "            \n",
    "            if len(result_list) > 0:\n",
    "                last_successful_offset = offset + len(result_list)\n",
    "                consecutive_empty = 0\n",
    "                print(f\"    ✓ Got {len(result_list)} records\")\n",
    "                \n",
    "                # Jump ahead based on what we got\n",
    "                if len(result_list) == batch_size:\n",
    "                    # Full batch, jump ahead more\n",
    "                    offset += batch_size * 10\n",
    "                else:\n",
    "                    # Partial batch, we're near the end\n",
    "                    offset += batch_size\n",
    "            else:\n",
    "                consecutive_empty += 1\n",
    "                print(f\"    ❌ Empty batch ({consecutive_empty}/3)\")\n",
    "                offset += batch_size\n",
    "                \n",
    "            time.sleep(0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "            consecutive_empty += 1\n",
    "            offset += batch_size\n",
    "            \n",
    "        # Safety limit\n",
    "        if offset > 2000000:\n",
    "            print(\"  Reached safety limit of 2M\")\n",
    "            break\n",
    "    \n",
    "    print(f\"  Progressive estimate: ~{last_successful_offset:,} records\")\n",
    "    return last_successful_offset\n",
    "\n",
    "def estimate_download_specs(record_count):\n",
    "    \"\"\"\n",
    "    Estimate download time and file size\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== DOWNLOAD ESTIMATES FOR {record_count:,.0f} RECORDS ===\")\n",
    "    \n",
    "    # File size estimates (bytes per record)\n",
    "    bytes_per_record_low = 150   # Minimal record\n",
    "    bytes_per_record_high = 600  # Detailed record\n",
    "    \n",
    "    size_mb_low = (record_count * bytes_per_record_low) / (1024 * 1024)\n",
    "    size_mb_high = (record_count * bytes_per_record_high) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Estimated file size: {size_mb_low:.1f} - {size_mb_high:.1f} MB\")\n",
    "    print(f\"                    ({size_mb_low/1024:.2f} - {size_mb_high/1024:.2f} GB)\")\n",
    "    \n",
    "    # Time estimates for different speeds\n",
    "    speeds = [\n",
    "        (500, \"Conservative\"),\n",
    "        (1000, \"Typical\"),\n",
    "        (2000, \"Optimistic\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nEstimated download times:\")\n",
    "    for speed, label in speeds:\n",
    "        time_seconds = record_count / speed\n",
    "        time_minutes = time_seconds / 60\n",
    "        time_hours = time_minutes / 60\n",
    "        \n",
    "        if time_hours >= 1:\n",
    "            print(f\"  {label} ({speed} rec/sec): {time_hours:.1f} hours\")\n",
    "        else:\n",
    "            print(f\"  {label} ({speed} rec/sec): {time_minutes:.0f} minutes\")\n",
    "    \n",
    "    # Batch size recommendations\n",
    "    print(f\"\\nRecommended batch sizes:\")\n",
    "    for batch_size in [1000, 5000, 10000]:\n",
    "        num_batches = (record_count + batch_size - 1) // batch_size\n",
    "        print(f\"  {batch_size:,} records/batch: {num_batches:,} API calls\")\n",
    "\n",
    "def quick_sample_check():\n",
    "    \"\"\"\n",
    "    Quick check to see if the query works and what data looks like\n",
    "    \"\"\"\n",
    "    print(\"\\n=== QUICK DATA SAMPLE ===\")\n",
    "    \n",
    "    assay = new_client.assay\n",
    "    \n",
    "    try:\n",
    "        print(\"Getting small sample to check data quality...\")\n",
    "        \n",
    "        sample = assay.filter(\n",
    "            assay_type='A',\n",
    "            assay_organism__iexact='Homo sapiens'\n",
    "        ).only(['assay_type', 'description', 'assay_chembl_id', 'assay_organism'])[:5]\n",
    "        \n",
    "        sample_list = list(sample)\n",
    "        \n",
    "        print(f\"Sample size: {len(sample_list)} records\")\n",
    "        \n",
    "        if sample_list:\n",
    "            print(\"\\nSample records:\")\n",
    "            for i, record in enumerate(sample_list, 1):\n",
    "                print(f\"  {i}. {record.get('assay_chembl_id', 'N/A')}\")\n",
    "                desc = record.get('description', 'No description')\n",
    "                if len(desc) > 80:\n",
    "                    desc = desc[:80] + \"...\"\n",
    "                print(f\"     {desc}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"❌ No records found with your filter criteria!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting sample: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ChEMBL Assay Size Estimator\")\n",
    "    print(\"Since direct counting isn't available, we'll use estimation methods\")\n",
    "    print()\n",
    "    \n",
    "    # Quick sample first\n",
    "    quick_sample_check()\n",
    "    \n",
    "    # Main estimation\n",
    "    estimates = estimate_chembl_assays()\n",
    "    \n",
    "    print(\"\\n=== NEXT STEPS ===\")\n",
    "    if estimates:\n",
    "        avg_estimate = sum(estimates) / len(estimates)\n",
    "        if avg_estimate < 50000:\n",
    "            print(\"✅ Dataset seems manageable - you can download it relatively quickly\")\n",
    "            print(\"💡 Suggest starting with your 100K test, then full dataset\")\n",
    "        elif avg_estimate < 200000:\n",
    "            print(\"⚠️  Medium-sized dataset - will take some time\")\n",
    "            print(\"💡 Suggest using streaming approach with checkpoints\")\n",
    "        else:\n",
    "            print(\"🔥 Large dataset - plan accordingly\")\n",
    "            print(\"💡 Definitely use checkpointing and consider running overnight\")\n",
    "    \n",
    "    print(\"\\nRun your 100K test first to validate these estimates!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fa94d-9945-4020-9144-094083618a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "def conservative_chembl_query():\n",
    "    \"\"\"\n",
    "    Very conservative ChEMBL query that respects rate limits\n",
    "    Based on your experience: after ~5K records, you get throttled to 20 records/batch\n",
    "    \"\"\"\n",
    "    # Ultra-conservative parameters\n",
    "    initial_batch_size = 1000  # Start smaller\n",
    "    throttled_batch_size = 20   # What we get when throttled\n",
    "    max_records = 10000         # Much smaller test - just 10K records\n",
    "    rawfile = \"../data/raw/chembl_assays_human_A_conservative_10k.csv\"\n",
    "    \n",
    "    # Longer delays to avoid rate limiting\n",
    "    normal_delay = 0.1          # 2 seconds between normal requests\n",
    "    throttled_delay = 1.0       # 5 seconds when we detect throttling\n",
    "    \n",
    "    assay = new_client.assay\n",
    "    os.makedirs(os.path.dirname(rawfile), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(rawfile):\n",
    "        os.remove(rawfile)\n",
    "    \n",
    "    total_records = 0\n",
    "    offset = 0\n",
    "    first_write = True\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Track throttling\n",
    "    is_throttled = False\n",
    "    consecutive_small_batches = 0\n",
    "    \n",
    "    print(\"=== ChEMBL Conservative Query (10K records) ===\")\n",
    "    print(f\"Target: {max_records:,} records\")\n",
    "    print(f\"Initial batch size: {initial_batch_size:,}\")\n",
    "    print(f\"Normal delay: {normal_delay}s, Throttled delay: {throttled_delay}s\")\n",
    "    print(f\"Output file: {rawfile}\")\n",
    "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    while offset < max_records:\n",
    "        # Determine batch size based on throttling status\n",
    "        if is_throttled:\n",
    "            current_batch_size = throttled_batch_size\n",
    "            delay = throttled_delay\n",
    "        else:\n",
    "            current_batch_size = min(initial_batch_size, max_records - offset)\n",
    "            delay = normal_delay\n",
    "        \n",
    "        print(f\"Fetching batch {offset:,} to {offset + current_batch_size:,}...\")\n",
    "        print(f\"  Mode: {'THROTTLED' if is_throttled else 'NORMAL'}\")\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            batch = assay.filter(\n",
    "                assay_type='A',\n",
    "                assay_organism__iexact='Homo sapiens'\n",
    "            ).only(['assay_type', 'description', 'assay_chembl_id', 'assay_organism'])[offset:offset + current_batch_size]\n",
    "            \n",
    "            batch_list = list(batch)\n",
    "            batch_size_actual = len(batch_list)\n",
    "            \n",
    "            if batch_size_actual == 0:\n",
    "                print(\"  No more records - stopping\")\n",
    "                break\n",
    "            \n",
    "            # Detect throttling\n",
    "            if not is_throttled and batch_size_actual < current_batch_size * 0.5:\n",
    "                # Got significantly fewer records than requested\n",
    "                consecutive_small_batches += 1\n",
    "                print(f\"  ⚠️  Small batch detected ({batch_size_actual}/{current_batch_size}) - count: {consecutive_small_batches}\")\n",
    "                \n",
    "                if consecutive_small_batches >= 2:\n",
    "                    print(\"  🚨 THROTTLING DETECTED - switching to conservative mode\")\n",
    "                    is_throttled = True\n",
    "                    consecutive_small_batches = 0\n",
    "            else:\n",
    "                consecutive_small_batches = 0\n",
    "            \n",
    "            # Write data\n",
    "            df_batch = pd.DataFrame(batch_list)\n",
    "            df_batch.to_csv(rawfile, mode='a', header=first_write, index=False)\n",
    "            first_write = False\n",
    "            \n",
    "            total_records += batch_size_actual\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            \n",
    "            # Progress info\n",
    "            elapsed_time = time.time() - start_time\n",
    "            records_per_second = total_records / elapsed_time if elapsed_time > 0 else 0\n",
    "            progress_percent = (total_records / max_records) * 100\n",
    "            \n",
    "            print(f\"  ✓ Got {batch_size_actual:,} records in {batch_time:.1f}s\")\n",
    "            print(f\"  Progress: {total_records:,}/{max_records:,} ({progress_percent:.1f}%)\")\n",
    "            print(f\"  Overall rate: {records_per_second:.0f} records/sec\")\n",
    "            \n",
    "            # ETA calculation\n",
    "            if records_per_second > 0:\n",
    "                remaining_records = max_records - total_records\n",
    "                eta_seconds = remaining_records / records_per_second\n",
    "                eta_minutes = eta_seconds / 60\n",
    "                print(f\"  ETA: ~{eta_minutes:.1f} minutes\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "            print(f\"  Retrying in {delay * 2} seconds...\")\n",
    "            time.sleep(delay * 2)\n",
    "            continue\n",
    "        \n",
    "        offset += batch_size_actual\n",
    "        \n",
    "        # Respectful delay\n",
    "        print(f\"  Waiting {delay}s...\")\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    # Final summary\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=== COMPLETED ===\")\n",
    "    print(f\"Records retrieved: {total_records:,}\")\n",
    "    print(f\"Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    if total_time > 0:\n",
    "        print(f\"Average rate: {total_records/total_time:.0f} records/second\")\n",
    "    print(f\"Throttling detected: {'Yes' if is_throttled else 'No'}\")\n",
    "    print(f\"File: {rawfile}\")\n",
    "    \n",
    "    # Quick analysis\n",
    "    if total_records > 0:\n",
    "        analyze_data_quick(rawfile, total_records)\n",
    "    \n",
    "    return total_records\n",
    "\n",
    "def analyze_data_quick(filepath, record_count):\n",
    "    \"\"\"\n",
    "    Quick analysis of the retrieved data\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== QUICK ANALYSIS ===\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        file_size_mb = os.path.getsize(filepath) / 1024 / 1024\n",
    "        \n",
    "        print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"Records per MB: ~{record_count / file_size_mb:.0f}\")\n",
    "        \n",
    "        # Extrapolate for larger datasets\n",
    "        print(f\"\\nExtrapolation:\")\n",
    "        for target_size in [50000, 100000, 500000]:\n",
    "            estimated_mb = target_size / (record_count / file_size_mb)\n",
    "            estimated_time_hours = (target_size / record_count) * (time.time() - start_time) / 3600\n",
    "            print(f\"  {target_size:,} records → ~{estimated_mb:.0f} MB, ~{estimated_time_hours:.1f} hours\")\n",
    "        \n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df.head(2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {e}\")\n",
    "\n",
    "def estimate_full_dataset_time():\n",
    "    \"\"\"\n",
    "    Based on your throttling experience, estimate time for full dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n=== FULL DATASET TIME ESTIMATES ===\")\n",
    "    print(\"Based on your throttling experience:\")\n",
    "    print()\n",
    "    \n",
    "    # Your observed performance\n",
    "    normal_rate = 5000 / 0.3  # ~16,667 records/sec for first batch\n",
    "    throttled_rate = 20 / 0.2  # ~100 records/sec when throttled\n",
    "    \n",
    "    # Assume we get throttled after 5K records\n",
    "    throttle_threshold = 5000\n",
    "    \n",
    "    for dataset_size in [10000, 50000, 100000, 500000]:\n",
    "        if dataset_size <= throttle_threshold:\n",
    "            # All at normal rate\n",
    "            time_seconds = dataset_size / normal_rate\n",
    "        else:\n",
    "            # First 5K at normal rate, rest at throttled rate\n",
    "            normal_time = throttle_threshold / normal_rate\n",
    "            throttled_records = dataset_size - throttle_threshold\n",
    "            throttled_time = throttled_records / throttled_rate\n",
    "            time_seconds = normal_time + throttled_time\n",
    "        \n",
    "        time_minutes = time_seconds / 60\n",
    "        time_hours = time_minutes / 60\n",
    "        \n",
    "        if time_hours >= 1:\n",
    "            print(f\"  {dataset_size:,} records: ~{time_hours:.1f} hours\")\n",
    "        else:\n",
    "            print(f\"  {dataset_size:,} records: ~{time_minutes:.0f} minutes\")\n",
    "    \n",
    "    print()\n",
    "    print(\"💡 Recommendations:\")\n",
    "    print(\"  - Start with 10K records to validate approach\")\n",
    "    print(\"  - For larger datasets, consider running overnight\")\n",
    "    print(\"  - Monitor for throttling and adjust delays accordingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aaf1d5-02d7-4c26-84bb-9a7da05f69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ChEMBL Conservative Query\")\n",
    "    print(\"Designed to work with rate limiting based on your experience\")\n",
    "    print()\n",
    "    \n",
    "    # Show time estimates first\n",
    "    estimate_full_dataset_time()\n",
    "    \n",
    "    print()\n",
    "    choice = input(\"Proceed with 10K record test? (y/n): \")\n",
    "    \n",
    "    if choice.lower() == 'y':\n",
    "        start_time = time.time()\n",
    "        total = conservative_chembl_query()\n",
    "        \n",
    "        if total > 0:\n",
    "            print(f\"\\n✅ Successfully retrieved {total:,} records!\")\n",
    "        else:\n",
    "            print(\"\\n❌ No data retrieved\")\n",
    "    else:\n",
    "        print(\"Cancelled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97eb1519-e889-4626-94ee-2c5a7fe45bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ChEMBL Fixed Throttle Query ===\n",
      "Records target: 10000\n",
      "Batch size: 50\n",
      "Delay per batch: 0.5s\n",
      "Output file: ../data/raw/chembl_assays_human_A_throttled_10k.csv\n",
      "Started at: 2025-06-17 13:13:42\n",
      "\n",
      "Fetching records 0 to 50...\n",
      "  ✓ Got 50 records. Total: 50\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 50 to 100...\n",
      "  ✓ Got 20 records. Total: 70\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 70 to 120...\n",
      "  ✓ Got 20 records. Total: 90\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 90 to 140...\n",
      "  ✓ Got 20 records. Total: 110\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 110 to 160...\n",
      "  ✓ Got 20 records. Total: 130\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 130 to 180...\n",
      "  ✓ Got 20 records. Total: 150\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 150 to 200...\n",
      "  ✓ Got 20 records. Total: 170\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 170 to 220...\n",
      "  ✓ Got 20 records. Total: 190\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 190 to 240...\n",
      "  ✓ Got 20 records. Total: 210\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 210 to 260...\n",
      "  ✓ Got 20 records. Total: 230\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 230 to 280...\n",
      "  ✓ Got 20 records. Total: 250\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 250 to 300...\n",
      "  ✓ Got 20 records. Total: 270\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 270 to 320...\n",
      "  ✓ Got 20 records. Total: 290\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 290 to 340...\n",
      "  ✓ Got 20 records. Total: 310\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 310 to 360...\n",
      "  ✓ Got 20 records. Total: 330\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 330 to 380...\n",
      "  ✓ Got 20 records. Total: 350\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 350 to 400...\n",
      "  ✓ Got 20 records. Total: 370\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 370 to 420...\n",
      "  ✓ Got 20 records. Total: 390\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 390 to 440...\n",
      "  ✓ Got 20 records. Total: 410\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 410 to 460...\n",
      "  ✓ Got 20 records. Total: 430\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 430 to 480...\n",
      "  ✓ Got 20 records. Total: 450\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 450 to 500...\n",
      "  ✓ Got 20 records. Total: 470\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 470 to 520...\n",
      "  ✓ Got 20 records. Total: 490\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 490 to 540...\n",
      "  ✓ Got 20 records. Total: 510\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 510 to 560...\n",
      "  ✓ Got 20 records. Total: 530\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 530 to 580...\n",
      "  ✓ Got 20 records. Total: 550\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 550 to 600...\n",
      "  ✓ Got 20 records. Total: 570\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 570 to 620...\n",
      "  ✓ Got 20 records. Total: 590\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 590 to 640...\n",
      "  ✓ Got 20 records. Total: 610\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 610 to 660...\n",
      "  ✓ Got 20 records. Total: 630\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 630 to 680...\n",
      "  ✓ Got 20 records. Total: 650\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 650 to 700...\n",
      "  ✓ Got 20 records. Total: 670\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 670 to 720...\n",
      "  ✓ Got 20 records. Total: 690\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 690 to 740...\n",
      "  ✓ Got 20 records. Total: 710\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 710 to 760...\n",
      "  ✓ Got 20 records. Total: 730\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 730 to 780...\n",
      "  ✓ Got 20 records. Total: 750\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 750 to 800...\n",
      "  ✓ Got 20 records. Total: 770\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 770 to 820...\n",
      "  ✓ Got 20 records. Total: 790\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 790 to 840...\n",
      "  ✓ Got 20 records. Total: 810\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 810 to 860...\n",
      "  ✓ Got 20 records. Total: 830\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 830 to 880...\n",
      "  ✓ Got 20 records. Total: 850\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n",
      "Fetching records 850 to 900...\n",
      "  ✓ Got 20 records. Total: 870\n",
      "  Sleeping 0.5s to avoid throttling...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_records\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[43mchembl_query_throttle_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mchembl_query_throttle_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Sleeping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms to avoid throttling...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m=== DONE ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "def chembl_query_throttle_safe():\n",
    "    \"\"\"\n",
    "    Conservative ChEMBL query that uses fixed small batches and long delays\n",
    "    to respect known throttling behavior.\n",
    "    \"\"\"\n",
    "    batch_size = 50\n",
    "    max_records = 10000\n",
    "    delay = 0.5  # seconds between requests\n",
    "    rawfile = \"../data/raw/chembl_assays_human_A_throttled_10k.csv\"\n",
    "\n",
    "    assay = new_client.assay\n",
    "    os.makedirs(os.path.dirname(rawfile), exist_ok=True)\n",
    "    if os.path.exists(rawfile):\n",
    "        os.remove(rawfile)\n",
    "\n",
    "    offset = 0\n",
    "    total_records = 0\n",
    "    first_write = True\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"=== ChEMBL Fixed Throttle Query ===\")\n",
    "    print(f\"Records target: {max_records}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Delay per batch: {delay}s\")\n",
    "    print(f\"Output file: {rawfile}\")\n",
    "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    while total_records < max_records:\n",
    "        print(f\"Fetching records {offset} to {offset + batch_size}...\")\n",
    "        try:\n",
    "            batch = assay.filter(\n",
    "                assay_type='A',\n",
    "                assay_organism__iexact='Homo sapiens'\n",
    "            ).only(['assay_type', 'description', 'assay_chembl_id', 'assay_organism'])[offset:offset + batch_size]\n",
    "\n",
    "            data = list(batch)\n",
    "            if not data:\n",
    "                print(\"No more data returned.\")\n",
    "                break\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(rawfile, mode='a', header=first_write, index=False)\n",
    "            first_write = False\n",
    "\n",
    "            total_records += len(df)\n",
    "            offset += len(df)\n",
    "\n",
    "            print(f\"  ✓ Got {len(df)} records. Total: {total_records}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "            print(f\"  Waiting extra {delay*2}s before retrying...\")\n",
    "            time.sleep(delay * 2)\n",
    "            continue\n",
    "\n",
    "        print(f\"  Sleeping {delay}s to avoid throttling...\\n\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"=== DONE ===\")\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Total time: {elapsed/60:.1f} minutes\")\n",
    "    return total_records\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chembl_query_throttle_safe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a45949-b912-4476-b6d1-3e4c70117c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
